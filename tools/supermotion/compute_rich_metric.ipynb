{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir('../../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import imageio\n",
    "from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines\n",
    "from hmr4d.dataset.rich.rich_utils import get_w2az_sahmr, parse_seqname_info\n",
    "from hmr4d.utils.geo_transform import apply_T_on_points\n",
    "from hmr4d.utils.eval.wham.eval_utils import convert_joints22_to_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "wham_rich = torch.load(\"inputs/RICH/eval_support/wham_rich.pt\")\n",
    "labels = np.load(\"inputs/RICH/eval_support/rich_test_vit.pth\", allow_pickle=True)\n",
    "vid2labelindex = {vid: i for i, vid in enumerate(labels[\"vid\"])}\n",
    "wham_rich_world = torch.load(\"inputs/RICH/eval_support/sm_rich_mocap_input.pt\")[\"vid_to_pred_j3d_ayfz1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.geo_transform import compute_T_ayfz2ay, compute_T_ayf2az\n",
    "from hmr4d.dataset.rich.rich_utils import RichVid2Tc2az\n",
    "\n",
    "compute_Tc2az_from_vid = RichVid2Tc2az()\n",
    "\n",
    "all_dump = torch.load(\"dump.pt\")\n",
    "# vid_to_indices = defaultdict(list)\n",
    "# for i, dump in enumerate(all_dump):\n",
    "#     vid = dump[\"meta\"][0]\n",
    "#     vid_to_indices[vid].append(i)\n",
    "# print(\"Number of clips: \", len(all_dump))\n",
    "# print(\"Number of videos: \", len(vid_to_indices))\n",
    "# print(all_dump[0].keys())\n",
    "\n",
    "gt_vidse2motion = {}\n",
    "wham_vidse2motion = {}\n",
    "ours_vidse2motion = {}\n",
    "for dump in all_dump:\n",
    "    vid, (s, e) = dump[\"meta\"]\n",
    "    vidse = f\"{vid}_{int(s)}_{int(e)}\"\n",
    "\n",
    "    # gt\n",
    "    j = vid2labelindex[vid]\n",
    "    gt_c = labels[\"joints3D\"][j][1:][s:e]  # (L, 24, 3)\n",
    "    T_c2az = compute_Tc2az_from_vid(vid)\n",
    "    gt_az = apply_T_on_points(gt_c, T_c2az)\n",
    "    T_az2ayfz = compute_T_ayf2az(gt_az[:1], inverse=True)[0]  # (4, 4)\n",
    "    gt_ayfz = apply_T_on_points(gt_az, T_az2ayfz)  # (L, 24, 3)\n",
    "    gt_vidse2motion[vidse] = {\n",
    "        \"c\": gt_c,\n",
    "        \"w\": gt_ayfz,\n",
    "    }\n",
    "\n",
    "    # wham\n",
    "    wham_ay = wham_rich_world[vid][s:e]\n",
    "    T_ay2ayfz = compute_T_ayfz2ay(wham_ay[:1], inverse=True)[0]  # (4, 4)\n",
    "    wham_w = apply_T_on_points(wham_ay, T_ay2ayfz)  # (F, 22, 3)\n",
    "    wham_vidse2motion[vidse] = {\n",
    "        \"c\": wham_rich[vid][\"cam_motion3d\"][s:e],\n",
    "        \"w\": wham_w,\n",
    "    }\n",
    "\n",
    "    # ours\n",
    "    pred_ayfz = convert_joints22_to_24(dump[\"pred_ayfz_motion\"])\n",
    "    pred_T_ayfz2c = dump[\"pred_T_ayfz2c\"]\n",
    "    pred_c = apply_T_on_points(pred_ayfz, pred_T_ayfz2c)\n",
    "    ours_vidse2motion[vidse] = {\n",
    "        \"c\": pred_c,\n",
    "        \"w\": pred_ayfz,\n",
    "    }\n",
    "\n",
    "print(len(ours_vidse2motion))\n",
    "\n",
    "# wis3d = make_wis3d() \n",
    "# add_motion_as_lines(gt_vidse2motion[vidse]['w'], wis3d, name='gt_w')\n",
    "# add_motion_as_lines(wham_vidse2motion[vidse]['w'], wis3d, name='wham_w')\n",
    "# add_motion_as_lines(ours_vidse2motion[vidse]['w'], wis3d, name='ours_w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clips:  236\n"
     ]
    }
   ],
   "source": [
    "import torch.functional as F \n",
    "from pytorch3d.transforms import axis_angle_to_matrix\n",
    "#  ======= TRACE ======== #\n",
    "trace_dumps = torch.load(\"/nas/share/hmr4d/comp_exp/TRACE@RICH.pt\")\n",
    "trace_vidse2motion = {}\n",
    "yup2ydown = axis_angle_to_matrix(torch.tensor([[np.pi, 0, 0]])).float()\n",
    "\n",
    "for vid in trace_dumps:\n",
    "    for j in range(len(trace_dumps[vid])):\n",
    "        s = trace_dumps[vid][j]['start'] \n",
    "        e = trace_dumps[vid][j]['end']\n",
    "\n",
    "        vidse = f\"{vid}_{int(s)}_{int(e)}\"\n",
    "        motion3d = torch.from_numpy(trace_dumps[vid][j]['motion3d']).float()  # (F, 24, 3)\n",
    "        motion3d = torch.einsum(\"...jk,...lk->...jl\", motion3d, yup2ydown)\n",
    "        trace_vidse2motion[vidse] = {\"w\": motion3d}  # Cam coordinate\n",
    "print(\"Number of clips: \", len(trace_vidse2motion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Evaluation on RICH, PA_MPJPE: 43.4920, MPJPE: 81.6215, ACCEL: 5.4840,\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.eval.wham.eval_utils import batch_compute_similarity_transform_torch, compute_error_accel\n",
    "from collections import defaultdict\n",
    "\n",
    "m2mm = 1000\n",
    "accumulator = defaultdict(list)\n",
    "\n",
    "keys_to_evaluate = list(gt_vidse2motion.keys())\n",
    "for vidse in keys_to_evaluate:\n",
    "    gt_c = gt_vidse2motion[vidse][\"c\"]\n",
    "    pred_c = wham_vidse2motion[vidse][\"c\"]\n",
    "    # pred_c = ours_vidse2motion[vidse][\"c\"]\n",
    "\n",
    "    # MPJPE\n",
    "    gt_cr = gt_c - gt_c[:, [1, 2]].mean(-2, keepdim=True)\n",
    "    pred_cr = pred_c - pred_c[:, [1, 2]].mean(-2, keepdim=True)\n",
    "    mpjpe = torch.sqrt(((pred_cr - gt_cr) ** 2).sum(dim=-1)).mean(dim=-1).numpy() * m2mm\n",
    "\n",
    "    # PAMPJPE\n",
    "    S1_hat = batch_compute_similarity_transform_torch(pred_cr, gt_cr)\n",
    "    pa_mpjpe = torch.sqrt(((S1_hat - gt_cr) ** 2).sum(dim=-1)).mean(dim=-1).numpy() * m2mm\n",
    "\n",
    "    accel = compute_error_accel(joints_pred=pred_cr, joints_gt=gt_cr)[1:-1]\n",
    "    accel = accel * (30**2)  # per frame^s to per s^2\n",
    "\n",
    "    # Accumulate\n",
    "    accumulator[\"pa_mpjpe\"].append(pa_mpjpe)\n",
    "    accumulator[\"mpjpe\"].append(mpjpe)\n",
    "    accumulator[\"accel\"].append(accel)\n",
    "\n",
    "for k, v in accumulator.items():\n",
    "    accumulator[k] = np.concatenate(v).mean()\n",
    "\n",
    "log_str = \"Local Evaluation on RICH, \"\n",
    "log_str += \" \".join([f\"{k.upper()}: {v:.4f},\" for k, v in accumulator.items()])\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Evaluation on RICH, MPJPE: 343.5977, PA-MPJPE: 77.2557, WA2_MPJPE: 1489.9246, WAA_MPJPE: 254.7058,\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.eval.wham.eval_utils import batch_compute_similarity_transform_torch, compute_error_accel\n",
    "from hmr4d.utils.eval.wham.eval_utils import first_align_joints, global_align_joints, compute_jpe\n",
    "from collections import defaultdict\n",
    "\n",
    "m2mm = 1000\n",
    "accumulator = defaultdict(list)\n",
    "\n",
    "# keys_to_evaluate = list(gt_vidse2motion.keys())\n",
    "for vidse in keys_to_evaluate:\n",
    "    gt_w = gt_vidse2motion[vidse][\"w\"]\n",
    "    # pred_w = wham_vidse2motion[vidse][\"w\"]\n",
    "    # pred_w = ours_vidse2motion[vidse][\"w\"]\n",
    "    pred_w = trace_vidse2motion[vidse][\"w\"]\n",
    "\n",
    "    # MPJPE\n",
    "    gt_wr = gt_w - gt_w[:, [1, 2]].mean(-2, keepdim=True)\n",
    "    pred_wr = pred_w - pred_w[:, [1, 2]].mean(-2, keepdim=True)\n",
    "    mpjpe = compute_jpe(gt_wr, pred_wr) * m2mm\n",
    "\n",
    "    # PA-MPJPE\n",
    "    S1_hat = batch_compute_similarity_transform_torch(pred_wr, gt_wr)\n",
    "    pa_mpjpe = compute_jpe(S1_hat, gt_wr) * m2mm\n",
    "\n",
    "    # WA2-MPJPE and WAA-MPJPE\n",
    "    pred_w_a2 = first_align_joints(gt_w, pred_w)  # align first 2\n",
    "    pred_w_aa = global_align_joints(gt_w, pred_w)  # align all\n",
    "    wa2_jpe = compute_jpe(gt_w, pred_w_a2) * m2mm\n",
    "    waa_jpe = compute_jpe(gt_w, pred_w_aa) * m2mm\n",
    "\n",
    "    # Accumulate\n",
    "    accumulator[\"mpjpe\"].append(mpjpe)\n",
    "    accumulator[\"pa-mpjpe\"].append(pa_mpjpe)\n",
    "    accumulator[\"wa2_mpjpe\"].append(wa2_jpe)\n",
    "    accumulator[\"waa_mpjpe\"].append(waa_jpe)\n",
    "\n",
    "for k, v in accumulator.items():\n",
    "    accumulator[k] = np.concatenate(v).mean()\n",
    "\n",
    "log_str = \"Global Evaluation on RICH, \"\n",
    "log_str += \" \".join([f\"{k.upper()}: {v:.4f},\" for k, v in accumulator.items()])\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add WHAM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.geo_transform import compute_T_ayfz2ay\n",
    "from hmr4d.utils.eval.wham.eval_utils import first_align_joints, global_align_joints, compute_jpe\n",
    "\n",
    "prepared_conds = torch.load(\"inputs/RICH/eval_support/sm_rich_mocap_input.pt\")\n",
    "\n",
    "COMPUTE_METRIC = True\n",
    "gmpjpe = []\n",
    "mpjpe = []\n",
    "wmpjpe = []\n",
    "wampjpe = []\n",
    "\n",
    "for vid in vids:\n",
    "    vid_ = \"-\".join(vid.split(\"/\"))\n",
    "    # wis3d = make_wis3d(output_dir=wis3d_dir, name=f\"{vid_}\")\n",
    "\n",
    "    seq_length = len(vid2gt[vid])\n",
    "    chunk_length = 100\n",
    "    wham_motion_clips = []\n",
    "    for start in range(0, seq_length, chunk_length):\n",
    "        end = start + chunk_length\n",
    "        if seq_length - end < chunk_length:\n",
    "            end = seq_length\n",
    "        if end - start < chunk_length:\n",
    "            break\n",
    "\n",
    "        # Load motion\n",
    "        init_motion_ay = prepared_conds[\"vid_to_pred_j3d_ayfz1\"][vid][start:end, :22]  # not AYFZ when start!=0\n",
    "        T_ay2ayfz = compute_T_ayfz2ay(init_motion_ay[:1], inverse=True)[0]  # (4, 4)\n",
    "        init_motion_ayfz = apply_T_on_points(init_motion_ay, T_ay2ayfz)  # (F, 22, 3)\n",
    "        wham_motion_clips.append(init_motion_ayfz)\n",
    "\n",
    "        # Let's compute metric here\n",
    "        if COMPUTE_METRIC:\n",
    "            gt_ayfz_motion = vid2gt[vid][start:end]  # (F, 22, 3)\n",
    "            # gmpjpe\n",
    "            error = (init_motion_ayfz - gt_ayfz_motion).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            gmpjpe.append(error.mean())\n",
    "            # mpjpe\n",
    "            gt_ = gt_ayfz_motion - gt_ayfz_motion[:, :1]\n",
    "            pred_ = init_motion_ayfz - init_motion_ayfz[:, :1]\n",
    "            error = (pred_ - gt_).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            mpjpe.append(error.mean())\n",
    "\n",
    "            # wmpjpe and wampjpe\n",
    "            w_j3d = first_align_joints(gt_ayfz_motion, init_motion_ayfz)\n",
    "            wa_j3d = global_align_joints(gt_ayfz_motion, init_motion_ayfz)\n",
    "            w_jpe = compute_jpe(gt_ayfz_motion, w_j3d)\n",
    "            wa_jpe = compute_jpe(gt_ayfz_motion, wa_j3d)\n",
    "            wmpjpe.append(w_jpe.mean())\n",
    "            wampjpe.append(wa_jpe.mean())\n",
    "\n",
    "    # wham_motion = torch.cat(wham_motion_clips, dim=0)  # (F, 22, 3)\n",
    "    # add_motion_as_lines(wham_motion, wis3d, name=\"pred_wham\")\n",
    "\n",
    "if COMPUTE_METRIC:\n",
    "    print(f\"gmpjpe: {np.mean(gmpjpe) * 1000}\")\n",
    "    print(f\"mpjpe: {np.mean(mpjpe) * 1000}\")\n",
    "    print(f\"wmpjpe: {np.mean(wmpjpe) * 1000}\")\n",
    "    print(f\"wampjpe: {np.mean(wampjpe) * 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2pred = torch.load(pt_dir / \"vid2pred_ppp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.geo_transform import compute_T_ayfz2ay\n",
    "from hmr4d.utils.eval.wham.eval_utils import first_align_joints, global_align_joints, compute_jpe\n",
    "\n",
    "prepared_conds = torch.load(\"inputs/RICH/eval_support/sm_rich_mocap_input.pt\")\n",
    "\n",
    "vid2wham = {}\n",
    "for vid in vids:\n",
    "    seq_length = len(vid2gt[vid])\n",
    "    chunk_length = 100\n",
    "    wham_motion_clips = []\n",
    "    for start in range(0, seq_length, chunk_length):\n",
    "        end = start + chunk_length\n",
    "        if seq_length - end < chunk_length:\n",
    "            end = seq_length\n",
    "        if end - start < chunk_length:\n",
    "            break\n",
    "\n",
    "        # Load motion\n",
    "        init_motion_ay = prepared_conds[\"vid_to_pred_j3d_ayfz1\"][vid][start:end, :22]  # not AYFZ when start!=0\n",
    "        T_ay2ayfz = compute_T_ayfz2ay(init_motion_ay[:1], inverse=True)[0]  # (4, 4)\n",
    "        init_motion_ayfz = apply_T_on_points(init_motion_ay, T_ay2ayfz)  # (F, 22, 3)\n",
    "        wham_motion_clips.append(init_motion_ayfz)\n",
    "    wham_motion = torch.cat(wham_motion_clips, dim=0)  # (F, 22, 3)\n",
    "    vid2wham[vid] = wham_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(vid2gt, vid2pred):\n",
    "    gmpjpe = []\n",
    "    mpjpe = []\n",
    "    wmpjpe = []\n",
    "    wampjpe = []\n",
    "    clip2vid = []\n",
    "\n",
    "    vids = list(vid2gt.keys())\n",
    "    for vid in vids:\n",
    "        seq_length = len(vid2gt[vid])\n",
    "        chunk_length = 100\n",
    "        for start in range(0, seq_length, chunk_length):\n",
    "            end = start + chunk_length\n",
    "            if seq_length - end < chunk_length:\n",
    "                end = seq_length\n",
    "            if end - start < chunk_length:\n",
    "                break\n",
    "            clip2vid.append(vid)\n",
    "            gt = vid2gt[vid][start:end]  # (F, 22, 3)\n",
    "            pred = vid2pred[vid][start:end]\n",
    "\n",
    "            # gmpjpe\n",
    "            error = (pred - gt).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            gmpjpe.append(error.mean())\n",
    "            # mpjpe\n",
    "            gt_ = gt - gt[:, :1]\n",
    "            pred_ = pred - pred[:, :1]\n",
    "            error = (pred_ - gt_).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            mpjpe.append(error.mean())\n",
    "\n",
    "            # wmpjpe and wampjpe\n",
    "            w_j3d = first_align_joints(gt, pred)\n",
    "            wa_j3d = global_align_joints(gt, pred)\n",
    "            w_jpe = compute_jpe(gt, w_j3d)\n",
    "            wa_jpe = compute_jpe(gt, wa_j3d)\n",
    "            wmpjpe.append(w_jpe.mean())\n",
    "            wampjpe.append(wa_jpe.mean())\n",
    "\n",
    "    gmpjpe = np.array(gmpjpe) * 1000\n",
    "    mpjpe = np.array(mpjpe) * 1000\n",
    "    wmpjpe = np.array(wmpjpe) * 1000\n",
    "    wampjpe = np.array(wampjpe) * 1000\n",
    "    clip2vid = np.array(clip2vid) \n",
    "    return gmpjpe, mpjpe, wmpjpe, wampjpe, clip2vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmpjpe, mpjpe, wmpjpe, wampjpe, clip2vid = get_metrics(vid2gt, vid2pred)\n",
    "gmpjpe_pred = np.array([gmpjpe[clip2vid == vid].mean() for vid in vids])\n",
    "\n",
    "gmpjpe, mpjpe, wmpjpe, wampjpe, clip2vid = get_metrics(vid2gt, vid2wham)\n",
    "gmpjpe_wham = np.array([gmpjpe[clip2vid == vid].mean() for vid in vids])\n",
    "\n",
    "\n",
    "# print(f\"gmpjpe: {np.mean(gmpjpe) * 1000}\")\n",
    "# print(f\"mpjpe: {np.mean(mpjpe) * 1000}\")\n",
    "# print(f\"wmpjpe: {np.mean(wmpjpe) * 1000}\")\n",
    "# print(f\"wampjpe: {np.mean(wampjpe) * 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indices = np.argsort(gmpjpe_pred)\n",
    "print((gmpjpe_wham[pred_indices] - gmpjpe_pred[pred_indices]).astype(np.int32))\n",
    "\n",
    "# wham_indices = np.argsort(gmpjpe_wham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indices = np.argsort((gmpjpe_pred - gmpjpe_wham))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wis3d_dir = \"outputs/wis3d_ws\"  # visualization folder\n",
    "# Draw results\n",
    "for i in pred_indices[:30]:\n",
    "    vid = vids[i]\n",
    "    vid_ = \"-\".join(vid.split(\"/\"))\n",
    "    wis3d = make_wis3d(output_dir=wis3d_dir, name=f\"{vid_}\")\n",
    "    add_motion_as_lines(vid2gt[vid], wis3d, name=\"gt\", const_color='green')\n",
    "    add_motion_as_lines(vid2wham[vid], wis3d, name=\"wham\", const_color='red')\n",
    "    add_motion_as_lines(vid2pred[vid], wis3d, name=\"ours\", const_color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gmpjpe = [gmpjpe[clip2vid == vid].mean() for vid in vids]\n",
    "mpjpe = [mpjpe[clip2vid == vid].mean() for vid in vids]\n",
    "wmpjpe = [wmpjpe[clip2vid == vid].mean() for vid in vids]\n",
    "wampjpe = [wampjpe[clip2vid == vid].mean() for vid in vids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmpjpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archived Utilities\n",
    "1. Convert Joints22 to Joints24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3438) tensor(0.3345)\n"
     ]
    }
   ],
   "source": [
    "from smplx import SMPL\n",
    "bm_kwargs = {\n",
    "    \"model_path\": \"inputs/checkpoints/body_models/smpl\",\n",
    "    \"gender\": \"neutral\",\n",
    "    \"num_betas\": 10,\n",
    "}\n",
    "with torch.no_grad():\n",
    "    model = SMPL(**bm_kwargs)\n",
    "    joints = model().joints[0, :24]\n",
    "\n",
    "\n",
    "ratio2321 = (joints[23] - joints[21]).norm(2) / (joints[21] - joints[19]).norm(2)\n",
    "ratio2220 = (joints[22] - joints[20]).norm(2) / (joints[20] - joints[18]).norm(2)\n",
    "print(ratio2220, ratio2321)\n",
    "\n",
    "def convert_joints22_to_24(joints22, ratio2220=0.3438, ratio2321=0.3345):\n",
    "    L = joints22.size(0)\n",
    "    joints24 = torch.zeros(L, 24, 3)\n",
    "    joints24[:, :22] = joints22\n",
    "    joints24[:, 22] = joints22[:, 20] + ratio2220 * (joints22[:, 20] - joints22[:, 18])\n",
    "    joints24[:, 23] = joints22[:, 21] + ratio2321 * (joints22[:, 21] - joints22[:, 19])\n",
    "    return joints24\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir('../../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import imageio\n",
    "from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines\n",
    "from hmr4d.dataset.rich.rich_utils import get_w2az_sahmr, parse_seqname_info\n",
    "from hmr4d.utils.geo_transform import apply_T_on_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos:  191\n",
      "dict_keys(['meta', 'pred_ayfz_motion', 'pred_T_ayfz2c', 'gt_ayfz_motion', 'gt_T_ayfz2c', 'wham_ayfz_motion', 'wham_cr_motion'])\n"
     ]
    }
   ],
   "source": [
    "all_dump = torch.load('dump.pt')\n",
    "vid_to_indices = {}\n",
    "for i, dump in enumerate(all_dump):\n",
    "    vid = dump['meta'][0]\n",
    "    if vid not in vid_to_indices:\n",
    "        vid_to_indices[vid] = [i]\n",
    "    else:\n",
    "        vid_to_indices[vid].append(i)\n",
    "print('Number of videos: ', len(vid_to_indices))\n",
    "print(all_dump[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wham_rich = torch.load(\"inputs/RICH/eval_support/wham_rich.pt\")\n",
    "labels = np.load(\"inputs/RICH/eval_support/rich_test_vit.pth\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([775, 24, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, vid in enumerate(labels[\"vid\"]):\n",
    "    break\n",
    "\n",
    "wham_rich[vid]['cam_motion3d'].shape\n",
    "labels['joints3D'][i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3438) tensor(0.3345)\n"
     ]
    }
   ],
   "source": [
    "from smplx import SMPL\n",
    "bm_kwargs = {\n",
    "    \"model_path\": \"inputs/checkpoints/body_models/smpl\",\n",
    "    \"gender\": \"neutral\",\n",
    "    \"num_betas\": 10,\n",
    "}\n",
    "with torch.no_grad():\n",
    "    model = SMPL(**bm_kwargs)\n",
    "    joints = model().joints[0, :24]\n",
    "\n",
    "\n",
    "ratio2321 = (joints[23] - joints[21]).norm(2) / (joints[21] - joints[19]).norm(2)\n",
    "ratio2220 = (joints[22] - joints[20]).norm(2) / (joints[20] - joints[18]).norm(2)\n",
    "print(ratio2220, ratio2321)\n",
    "\n",
    "def convert_joints22_to_24(joints22, ratio2220=0.3438, ratio2321=0.3345):\n",
    "    L = joints22.size(0)\n",
    "    joints24 = torch.zeros(L, 24, 3)\n",
    "    joints24[:, :22] = joints22\n",
    "    joints24[:, 22] = joints22[:, 20] + ratio2220 * (joints22[:, 20] - joints22[:, 18])\n",
    "    joints24[:, 23] = joints22[:, 21] + ratio2321 * (joints22[:, 21] - joints22[:, 19])\n",
    "    return joints24\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on RICH, PA_MPJPE: 43.7911, MPJPE: 81.7253, ACCEL: 5.4871,\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.eval.wham.eval_utils import batch_compute_similarity_transform_torch, compute_error_accel\n",
    "# from hmr4d.utils.eval.wham.eval_utils import  first_align_joints, global_align_joints, compute_jpe\n",
    "from collections import defaultdict\n",
    "\n",
    "m2mm = 1000\n",
    "accumulator = defaultdict(list)\n",
    "\n",
    "vid2labelindex = {vid: i for i, vid in enumerate(labels[\"vid\"])}\n",
    "\n",
    "for dump in all_dump:\n",
    "    # pred_c = wham_rich[vid]['cam_motion3d']\n",
    "    # gt_c = labels['joints3D'][i][1:][:, :22]\n",
    "    \n",
    "    # gt_ayfz = dump[\"gt_ayfz_motion\"]\n",
    "    # gt_T_ayfz2c = dump[\"gt_T_ayfz2c\"]\n",
    "    # gt_c = apply_T_on_points(gt_ayfz, gt_T_ayfz2c)\n",
    "    vid, (start, end) =  dump['meta']\n",
    "    i = vid2labelindex[vid]\n",
    "    # gt_c = labels['joints3D'][i][1:][start:end, :22]\n",
    "    gt_c = labels['joints3D'][i][1:][start:end]\n",
    "\n",
    "    pred_ayfz = dump[\"pred_ayfz_motion\"]\n",
    "    pred_T_ayfz2c = dump[\"pred_T_ayfz2c\"]\n",
    "    pred_c = apply_T_on_points(pred_ayfz, pred_T_ayfz2c)\n",
    "    # pred_c = dump[\"wham_cr_motion\"]\n",
    "    \n",
    "    pred_c = convert_joints22_to_24(pred_c)\n",
    "\n",
    "\n",
    "\n",
    "    # MPJPE\n",
    "    gt_cr = gt_c - gt_c[:, [1, 2]].mean(-2, keepdim=True)\n",
    "    pred_cr = pred_c - pred_c[:, [1, 2]].mean(-2, keepdim=True)\n",
    "    mpjpe = torch.sqrt(((pred_cr - gt_cr) ** 2).sum(dim=-1)).mean(dim=-1).numpy() * m2mm\n",
    "\n",
    "    # PAMPJPE\n",
    "    S1_hat = batch_compute_similarity_transform_torch(pred_cr, gt_cr)\n",
    "    pa_mpjpe = torch.sqrt(((S1_hat - gt_cr) ** 2).sum(dim=-1)).mean(dim=-1).numpy() * m2mm\n",
    "\n",
    "    accel = compute_error_accel(joints_pred=pred_cr, joints_gt=gt_cr)[1:-1]\n",
    "    accel = accel * (30**2)  # per frame^s to per s^2\n",
    "\n",
    "    # Accumulate\n",
    "    accumulator[\"pa_mpjpe\"].append(pa_mpjpe)\n",
    "    accumulator[\"mpjpe\"].append(mpjpe)\n",
    "    accumulator[\"accel\"].append(accel)\n",
    "\n",
    "for k, v in accumulator.items():\n",
    "    accumulator[k] = np.concatenate(v).mean()\n",
    "\n",
    "log_str = \"Evaluation on RICH, \"\n",
    "log_str += \" \".join([f\"{k.upper()}: {v:.4f},\" for k, v in accumulator.items()])\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3315, -0.7871,  1.8555])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours (variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfixs = [\"gg\", \"ggp\", \"gp\", \"pp\", \"ppp\"]\n",
    "postfix = postfixs[1]\n",
    "\n",
    "vid2pred = torch.load(pt_dir / f\"vid2pred_{postfix}.pt\")\n",
    "for vid in vids:\n",
    "    vid_ = \"-\".join(vid.split(\"/\"))\n",
    "    wis3d = make_wis3d(output_dir=wis3d_dir, name=f\"{vid_}\")\n",
    "    add_motion_as_lines(vid2pred[vid], wis3d, name=f\"ours_{postfix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add WHAM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.geo_transform import compute_T_ayfz2ay\n",
    "from hmr4d.utils.eval.wham.eval_utils import first_align_joints, global_align_joints, compute_jpe\n",
    "\n",
    "prepared_conds = torch.load(\"inputs/RICH/eval_support/sm_rich_mocap_input.pt\")\n",
    "\n",
    "COMPUTE_METRIC = True\n",
    "gmpjpe = []\n",
    "mpjpe = []\n",
    "wmpjpe = []\n",
    "wampjpe = []\n",
    "\n",
    "for vid in vids:\n",
    "    vid_ = \"-\".join(vid.split(\"/\"))\n",
    "    # wis3d = make_wis3d(output_dir=wis3d_dir, name=f\"{vid_}\")\n",
    "\n",
    "    seq_length = len(vid2gt[vid])\n",
    "    chunk_length = 100\n",
    "    wham_motion_clips = []\n",
    "    for start in range(0, seq_length, chunk_length):\n",
    "        end = start + chunk_length\n",
    "        if seq_length - end < chunk_length:\n",
    "            end = seq_length\n",
    "        if end - start < chunk_length:\n",
    "            break\n",
    "\n",
    "        # Load motion\n",
    "        init_motion_ay = prepared_conds[\"vid_to_pred_j3d_ayfz1\"][vid][start:end, :22]  # not AYFZ when start!=0\n",
    "        T_ay2ayfz = compute_T_ayfz2ay(init_motion_ay[:1], inverse=True)[0]  # (4, 4)\n",
    "        init_motion_ayfz = apply_T_on_points(init_motion_ay, T_ay2ayfz)  # (F, 22, 3)\n",
    "        wham_motion_clips.append(init_motion_ayfz)\n",
    "\n",
    "        # Let's compute metric here\n",
    "        if COMPUTE_METRIC:\n",
    "            gt_ayfz_motion = vid2gt[vid][start:end]  # (F, 22, 3)\n",
    "            # gmpjpe\n",
    "            error = (init_motion_ayfz - gt_ayfz_motion).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            gmpjpe.append(error.mean())\n",
    "            # mpjpe\n",
    "            gt_ = gt_ayfz_motion - gt_ayfz_motion[:, :1]\n",
    "            pred_ = init_motion_ayfz - init_motion_ayfz[:, :1]\n",
    "            error = (pred_ - gt_).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            mpjpe.append(error.mean())\n",
    "\n",
    "            # wmpjpe and wampjpe\n",
    "            w_j3d = first_align_joints(gt_ayfz_motion, init_motion_ayfz)\n",
    "            wa_j3d = global_align_joints(gt_ayfz_motion, init_motion_ayfz)\n",
    "            w_jpe = compute_jpe(gt_ayfz_motion, w_j3d)\n",
    "            wa_jpe = compute_jpe(gt_ayfz_motion, wa_j3d)\n",
    "            wmpjpe.append(w_jpe.mean())\n",
    "            wampjpe.append(wa_jpe.mean())\n",
    "\n",
    "    # wham_motion = torch.cat(wham_motion_clips, dim=0)  # (F, 22, 3)\n",
    "    # add_motion_as_lines(wham_motion, wis3d, name=\"pred_wham\")\n",
    "\n",
    "if COMPUTE_METRIC:\n",
    "    print(f\"gmpjpe: {np.mean(gmpjpe) * 1000}\")\n",
    "    print(f\"mpjpe: {np.mean(mpjpe) * 1000}\")\n",
    "    print(f\"wmpjpe: {np.mean(wmpjpe) * 1000}\")\n",
    "    print(f\"wampjpe: {np.mean(wampjpe) * 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2pred = torch.load(pt_dir / \"vid2pred_ppp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.geo_transform import compute_T_ayfz2ay\n",
    "from hmr4d.utils.eval.wham.eval_utils import first_align_joints, global_align_joints, compute_jpe\n",
    "\n",
    "prepared_conds = torch.load(\"inputs/RICH/eval_support/sm_rich_mocap_input.pt\")\n",
    "\n",
    "vid2wham = {}\n",
    "for vid in vids:\n",
    "    seq_length = len(vid2gt[vid])\n",
    "    chunk_length = 100\n",
    "    wham_motion_clips = []\n",
    "    for start in range(0, seq_length, chunk_length):\n",
    "        end = start + chunk_length\n",
    "        if seq_length - end < chunk_length:\n",
    "            end = seq_length\n",
    "        if end - start < chunk_length:\n",
    "            break\n",
    "\n",
    "        # Load motion\n",
    "        init_motion_ay = prepared_conds[\"vid_to_pred_j3d_ayfz1\"][vid][start:end, :22]  # not AYFZ when start!=0\n",
    "        T_ay2ayfz = compute_T_ayfz2ay(init_motion_ay[:1], inverse=True)[0]  # (4, 4)\n",
    "        init_motion_ayfz = apply_T_on_points(init_motion_ay, T_ay2ayfz)  # (F, 22, 3)\n",
    "        wham_motion_clips.append(init_motion_ayfz)\n",
    "    wham_motion = torch.cat(wham_motion_clips, dim=0)  # (F, 22, 3)\n",
    "    vid2wham[vid] = wham_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(vid2gt, vid2pred):\n",
    "    gmpjpe = []\n",
    "    mpjpe = []\n",
    "    wmpjpe = []\n",
    "    wampjpe = []\n",
    "    clip2vid = []\n",
    "\n",
    "    vids = list(vid2gt.keys())\n",
    "    for vid in vids:\n",
    "        seq_length = len(vid2gt[vid])\n",
    "        chunk_length = 100\n",
    "        for start in range(0, seq_length, chunk_length):\n",
    "            end = start + chunk_length\n",
    "            if seq_length - end < chunk_length:\n",
    "                end = seq_length\n",
    "            if end - start < chunk_length:\n",
    "                break\n",
    "            clip2vid.append(vid)\n",
    "            gt = vid2gt[vid][start:end]  # (F, 22, 3)\n",
    "            pred = vid2pred[vid][start:end]\n",
    "\n",
    "            # gmpjpe\n",
    "            error = (pred - gt).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            gmpjpe.append(error.mean())\n",
    "            # mpjpe\n",
    "            gt_ = gt - gt[:, :1]\n",
    "            pred_ = pred - pred[:, :1]\n",
    "            error = (pred_ - gt_).pow(2).sum(-1).sqrt()  # (F, 22)\n",
    "            mpjpe.append(error.mean())\n",
    "\n",
    "            # wmpjpe and wampjpe\n",
    "            w_j3d = first_align_joints(gt, pred)\n",
    "            wa_j3d = global_align_joints(gt, pred)\n",
    "            w_jpe = compute_jpe(gt, w_j3d)\n",
    "            wa_jpe = compute_jpe(gt, wa_j3d)\n",
    "            wmpjpe.append(w_jpe.mean())\n",
    "            wampjpe.append(wa_jpe.mean())\n",
    "\n",
    "    gmpjpe = np.array(gmpjpe) * 1000\n",
    "    mpjpe = np.array(mpjpe) * 1000\n",
    "    wmpjpe = np.array(wmpjpe) * 1000\n",
    "    wampjpe = np.array(wampjpe) * 1000\n",
    "    clip2vid = np.array(clip2vid) \n",
    "    return gmpjpe, mpjpe, wmpjpe, wampjpe, clip2vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmpjpe, mpjpe, wmpjpe, wampjpe, clip2vid = get_metrics(vid2gt, vid2pred)\n",
    "gmpjpe_pred = np.array([gmpjpe[clip2vid == vid].mean() for vid in vids])\n",
    "\n",
    "gmpjpe, mpjpe, wmpjpe, wampjpe, clip2vid = get_metrics(vid2gt, vid2wham)\n",
    "gmpjpe_wham = np.array([gmpjpe[clip2vid == vid].mean() for vid in vids])\n",
    "\n",
    "\n",
    "# print(f\"gmpjpe: {np.mean(gmpjpe) * 1000}\")\n",
    "# print(f\"mpjpe: {np.mean(mpjpe) * 1000}\")\n",
    "# print(f\"wmpjpe: {np.mean(wmpjpe) * 1000}\")\n",
    "# print(f\"wampjpe: {np.mean(wampjpe) * 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indices = np.argsort(gmpjpe_pred)\n",
    "print((gmpjpe_wham[pred_indices] - gmpjpe_pred[pred_indices]).astype(np.int32))\n",
    "\n",
    "# wham_indices = np.argsort(gmpjpe_wham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indices = np.argsort((gmpjpe_pred - gmpjpe_wham))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wis3d_dir = \"outputs/wis3d_ws\"  # visualization folder\n",
    "# Draw results\n",
    "for i in pred_indices[:30]:\n",
    "    vid = vids[i]\n",
    "    vid_ = \"-\".join(vid.split(\"/\"))\n",
    "    wis3d = make_wis3d(output_dir=wis3d_dir, name=f\"{vid_}\")\n",
    "    add_motion_as_lines(vid2gt[vid], wis3d, name=\"gt\", const_color='green')\n",
    "    add_motion_as_lines(vid2wham[vid], wis3d, name=\"wham\", const_color='red')\n",
    "    add_motion_as_lines(vid2pred[vid], wis3d, name=\"ours\", const_color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gmpjpe = [gmpjpe[clip2vid == vid].mean() for vid in vids]\n",
    "mpjpe = [mpjpe[clip2vid == vid].mean() for vid in vids]\n",
    "wmpjpe = [wmpjpe[clip2vid == vid].mean() for vid in vids]\n",
    "wampjpe = [wampjpe[clip2vid == vid].mean() for vid in vids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmpjpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

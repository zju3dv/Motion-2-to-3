{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir('../../')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import imageio\n",
    "import decord\n",
    "import joblib\n",
    "from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines\n",
    "from hmr4d.dataset.rich.rich_utils import get_w2az_sahmr, parse_seqname_info\n",
    "from hmr4d.utils.geo_transform import apply_T_on_points\n",
    "from hmr4d.utils.vis.vis_kpts import draw_kpts_cv2\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bbox', 'res', 'vid', 'pose', 'betas', 'transl', 'kp2d', 'joints3D', 'frame_id', 'cam_poses', 'features', 'gender', 'init_kp3d', 'init_pose'])\n",
      "Num sequences in RICH: 191\n",
      "WHAM output keys in one item: dict_keys(['poses_body', 'betas', 'poses_root_world', 'trans_world', 'weak_joints2d'])\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "# rich_test_vit_path = \"/home/shenzehong/Code/WHAM/dataset/parsed_data/rich_test_vit.pth\"\n",
    "rich_test_vit_path = \"inputs/RICH/eval_support/rich_test_vit.pth\"\n",
    "rich_video_dir = Path(\"inputs/RICH/hmr4d_support/video\")\n",
    "wham_rich_output = Path(\"inputs/RICH/eval_support/wham_output.pt\")\n",
    "\n",
    "# Load\n",
    "labels = joblib.load(rich_test_vit_path)\n",
    "print(labels.keys())\n",
    "N_seq = len(labels[\"vid\"])\n",
    "print(\"Num sequences in RICH:\", N_seq)\n",
    "wham_outputs = torch.load(wham_rich_output)\n",
    "print(\"WHAM output keys in one item:\", wham_outputs[\"test/Gym_010_cooking1/cam_01\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval global-metric @ RICH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.smplx_utils import make_smplx\n",
    "\n",
    "smplx_models = {\n",
    "    \"male\": make_smplx(\"rich-smplx\", gender=\"male\").cuda().eval(),\n",
    "    \"female\": make_smplx(\"rich-smplx\", gender=\"female\").cuda().eval(),\n",
    "}\n",
    "smpl_models = {\"neutral\": make_smplx(\"smpl\", gender=\"neutral\").cuda().eval()}\n",
    "\n",
    "smplx2smpl = torch.load(\"hmr4d/utils/body_model/smplx2smpl_sparse.pt\").to_dense()\n",
    "smpl_J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:06<00:00, 28.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch3d.transforms import matrix_to_axis_angle, axis_angle_to_matrix\n",
    "pred_j3d_globs, target_j3d_globs = [], []\n",
    "for index in tqdm(range(N_seq)):\n",
    "    vid = labels[\"vid\"][index]\n",
    "\n",
    "    # # <======= Build groundtruth SMPL (from SMPLX)    =======> We can use Joints3D\n",
    "    # pose = labels[\"pose\"][index][1:]  # (F, 72)\n",
    "    # betas = labels[\"betas\"][index][1:]  # (F, 10)\n",
    "    # gender = labels[\"gender\"][index]  # str\n",
    "    # transl = labels[\"transl\"][index][1:]  # (F, 3), global\n",
    "    # cam_poses = labels[\"cam_poses\"][index][1:]  # (F, 3, 4)\n",
    "    # # joints3d = labels[\"joints3D\"][index][1:]  # (F, 24, 3)\n",
    "\n",
    "    # # forward\n",
    "    # params = {\n",
    "    #     \"body_pose\": pose[:, 3:-6].reshape(-1, 63),\n",
    "    #     \"global_orient\": matrix_to_axis_angle(cam_poses[:, :3, :3].transpose(2, 1) @ axis_angle_to_matrix(pose[:, :3])),\n",
    "    #     \"betas\": betas,\n",
    "    #     \"transl\": transl,\n",
    "    # }\n",
    "    # params = {k: v.cuda() for k, v in params.items()}\n",
    "    # smplx_out = smplx_models[gender](**params)\n",
    "    # smpl_verts = torch.matmul(smplx2smpl, smplx_out.vertices.cpu())\n",
    "    # target_j3d_glob = smpl_J_regressor @ smpl_verts  # (F, 24, 3)\n",
    "    # target_j3d_globs.append(target_j3d_glob)\n",
    "\n",
    "    # <======= Prediction\n",
    "    wham_pred = wham_outputs[vid]\n",
    "    # print(wham_pred.keys())\n",
    "\n",
    "    params = {\n",
    "        \"body_pose\": wham_pred['poses_body'].reshape(-1, 23, 3, 3),\n",
    "        \"global_orient\": wham_pred['poses_root_world'].reshape(-1, 1, 3, 3),\n",
    "        \"betas\": wham_pred['betas'].reshape(-1, 10),\n",
    "        \"transl\": wham_pred['trans_world'].reshape(-1, 3),\n",
    "    }\n",
    "    params = {k: v.cuda() for k, v in params.items()}\n",
    "    pred_smpl_out = smpl_models['neutral'].forward(**params, pose2rot=False)\n",
    "    pred_j3d_glob = smpl_J_regressor @ pred_smpl_out.vertices.cpu()  # (F, 24, 3)\n",
    "    pred_j3d_globs.append(pred_j3d_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.78987\n",
      "116.9217\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.eval.wham.eval_utils import first_align_joints, global_align_joints, compute_jpe\n",
    "\n",
    "m2mm = 1000\n",
    "chunk_length = 100\n",
    "\n",
    "pa_mpjpes, w_mpjpes, wa_mpjpes = [], [], []\n",
    "\n",
    "\n",
    "# for pred_j3d_glob, target_j3d_glob in zip(pred_j3d_globs, target_j3d_globs):\n",
    "for pred_j3d_glob, target_j3d_glob in zip(pred_j3d_globs, labels[\"joints3D\"]):\n",
    "    w_mpjpe, wa_mpjpe = [], []\n",
    "    seq_length = len(pred_j3d_glob)\n",
    "    target_j3d_glob = target_j3d_glob[1:]\n",
    "    for start in range(0, seq_length - chunk_length, chunk_length):\n",
    "        end = start + chunk_length\n",
    "        if start + 2 * chunk_length > seq_length:  # last one\n",
    "            end = seq_length\n",
    "\n",
    "        target_j3d = target_j3d_glob[start:end].clone().cpu()\n",
    "        pred_j3d = pred_j3d_glob[start:end].clone().cpu()\n",
    "\n",
    "        w_j3d = first_align_joints(target_j3d, pred_j3d)\n",
    "        wa_j3d = global_align_joints(target_j3d, pred_j3d)\n",
    "\n",
    "        w_jpe = compute_jpe(target_j3d, w_j3d)\n",
    "        wa_jpe = compute_jpe(target_j3d, wa_j3d)\n",
    "        w_mpjpe.append(w_jpe)\n",
    "        wa_mpjpe.append(wa_jpe)\n",
    "\n",
    "    w_mpjpe = np.concatenate(w_mpjpe) * m2mm\n",
    "    wa_mpjpe = np.concatenate(wa_mpjpe) * m2mm\n",
    "    w_mpjpes.append(w_mpjpe.mean())\n",
    "    wa_mpjpes.append(wa_mpjpe.mean())\n",
    "\n",
    "print(np.mean(w_mpjpe))\n",
    "print(np.mean(wa_mpjpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "wis3d = make_wis3d(name=\"wham_global_metric\")\n",
    "add_motion_as_lines(labels['joints3D'][0], wis3d, \"gt_c1\")\n",
    "add_motion_as_lines(pred_j3d_globs[0], wis3d, \"pred_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化WHAM的输出结果\n",
    "\n",
    "- joints2d is not perfectly aligned with image\n",
    "- the first frame of joints2d should be removed (use the second frame to replace it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joints2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "77\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "wham_outputs = torch.load(\"wham_rich.pt\")  # latest version\n",
    "\n",
    "for index in range(191):\n",
    "    vid = labels[\"vid\"][index]\n",
    "    if \"ParkingLot2_017_burpeejump1\" in vid:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/ParkingLot2_017_burpeejump1/cam_01\n",
      "dict_keys(['cam_motion3d', 'i_joints2d'])\n"
     ]
    }
   ],
   "source": [
    "index = 77\n",
    "vid = labels[\"vid\"][index]\n",
    "print(vid)\n",
    "wham_pred = wham_outputs[vid]\n",
    "print(wham_pred.keys())\n",
    "\n",
    "# From dataset\n",
    "# cam_poses = labels[\"cam_poses\"][index][1:]  # (F, 3, 4)\n",
    "# joints3d = labels[\"joints3D\"][index][1:]  # (F, 24, 3)\n",
    "\n",
    "frame_id = labels[\"frame_id\"][index][1:]  # (F, )\n",
    "vr = decord.VideoReader(str(rich_video_dir / vid / \"video.mp4\"))\n",
    "frames = vr.get_batch(list(frame_id.numpy()))  # (F, 752, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints2d = wham_pred[\"i_joints2d\"].numpy()\n",
    "joints2d = joints2d / 4\n",
    "joints2d[:1] = joints2d[[1]]  # change 1\n",
    "\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "video_path = rich_video_dir / labels[\"vid\"][index] / \"video.mp4\"\n",
    "vr = decord.VideoReader(str(video_path))\n",
    "frames = vr.get_batch(list(frame_id.numpy()))  # (F, 752, 1024, 3)\n",
    "\n",
    "imgs = []\n",
    "for i in range(len(frames)):\n",
    "    img = frames[i].numpy().copy()\n",
    "    img = draw_kpts_cv2(img, joints2d[i])\n",
    "    imgs.append(img)\n",
    "imageio.mimsave(\"tmp_weakjoints2d.mp4\", imgs, fps=30, quality=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joints3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints3d = wham_pred[\"cam_motion3d\"]  # (F, 24, 3)\n",
    "\n",
    "from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines\n",
    "wis3d = make_wis3d(name=\"wham_cam_motion3d\")\n",
    "add_motion_as_lines(joints3d, wis3d, name=\"pred_cam_motion3d\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check groundtruth from WHAM-provided-pth (YES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs/RICH/hmr4d_support/video/test/Gym_012_lunge1/cam_04\n",
      "tensor([  4,   4, 203])\n",
      "201\n",
      "inputs/RICH/hmr4d_support/video/test/Gym_012_lunge1/cam_05\n",
      "tensor([  3,   3, 202])\n",
      "201\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# for index in range(N_seq):\n",
    "#     vid = labels[\"vid\"][index]\n",
    "#     video_path = rich_video_dir / labels[\"vid\"][index] / \"video.mp4\"\n",
    "\n",
    "#     meta_txt = rich_video_dir / vid / \"meta.txt\"  # 表示images_ds4里，第一个和最后一个jpg的文件名，比如[2, 255]一共有254帧，一个seq的不同cam可能不一样\n",
    "#     sfid, efid = [int(x) for x in meta_txt.open(\"r\").readline().split()]\n",
    "\n",
    "#     if sfid != 1:\n",
    "#         print(vid)\n",
    "#         print(index)\n",
    "#         break\n",
    "print(rich_video_dir / labels[\"vid\"][37])  #  jpg: 1, 255\n",
    "print(labels[\"frame_id\"][37][[0, 1, -1]])  # tensor([  4,   4, 203])\n",
    "print(len(labels[\"joints3D\"][37]))  # 201， 相机系下的\n",
    "print(rich_video_dir / labels[\"vid\"][6])  # jpg: 2, 255，jpg就少一张，但是一个jpg-id对应的local-pose肯定是一样的\n",
    "print(\n",
    "    labels[\"frame_id\"][6][[0, 1, -1]]\n",
    ")  # tensor([  3,   3,  202])  # 因为labels[\"pose\"]的local部分是一样的，说明这个frame-id是不是真的frame-id，需要加offset\n",
    "print(len(labels[\"joints3D\"][6]))  # 201, 相机系下的\n",
    "\n",
    "# labels[\"pose\"]的local部分是一样的\n",
    "print((labels[\"pose\"][6][:, 3:] != labels[\"pose\"][37][:, 3:]).sum())\n",
    "# global_orient是到相机系下的，transl可能是原本的世界系下的，因为这里两个是一样的\n",
    "print((labels[\"transl\"][6][1:] != labels[\"transl\"][37][1:]).sum())\n",
    "\n",
    "# # joints3D是cam-coord下的\n",
    "# wis3d = make_wis3d(name=\"rich_test\")\n",
    "# add_motion_as_lines(labels['joints3D'][6], wis3d, name='index6', skeleton_type='smpl22')\n",
    "# add_motion_as_lines(labels['joints3D'][37], wis3d, name='index37', skeleton_type='smpl22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.dataset.rich.rich_utils import get_cam2params, get_cam_key_wham_vid\n",
    "\n",
    "cam2params = get_cam2params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_key = get_cam_key_wham_vid(vid)\n",
    "T_w2c, K = cam2params[cam_key]\n",
    "\n",
    "frame_id = labels[\"frame_id\"][index][1:]  # (F, )\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "vr = decord.VideoReader(str(video_path))\n",
    "frames = vr.get_batch(list(frame_id.numpy()))  # (F, 752, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.smplx_utils import make_smplx\n",
    "from pytorch3d.transforms import axis_angle_to_matrix, matrix_to_axis_angle\n",
    "\n",
    "\n",
    "print(index)\n",
    "pose = labels[\"pose\"][index][1:]  # (F, 72)\n",
    "betas = labels[\"betas\"][index][1:]  # (F, 10)\n",
    "gender = labels[\"gender\"][index]  # str\n",
    "\n",
    "transl = labels[\"transl\"][index][1:]  # (F, 3), global\n",
    "cam_poses = labels[\"cam_poses\"][index][1:]  # (F, 3, 4)\n",
    "joints3d = labels[\"joints3D\"][index][1:]  # (F, 24, 3)\n",
    "\n",
    "body_pose = pose[:, 3:-6].reshape(-1, 63)\n",
    "global_orient = matrix_to_axis_angle(cam_poses[:, :3, :3].transpose(2, 1) @ axis_angle_to_matrix(pose[:, :3]))\n",
    "\n",
    "\n",
    "# <======= Build groundtruth SMPL (from SMPLX)\n",
    "smplx2smpl = torch.load(\"hmr4d/utils/body_model/smplx2smpl_sparse.pt\").to_dense()\n",
    "smpl_J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\")\n",
    "\n",
    "smplx_models = {\n",
    "    \"male\": make_smplx(\"rich-smplx\", gender=\"male\"),\n",
    "    \"female\": make_smplx(\"rich-smplx\", gender=\"female\"),\n",
    "}\n",
    "smplx_out = smplx_models[gender](\n",
    "    body_pose=pose[:, 3:-6].reshape(-1, 63),\n",
    "    global_orient=global_orient,\n",
    "    transl=transl,\n",
    "    betas=betas,\n",
    ")\n",
    "smpl_verts = torch.matmul(smplx2smpl, smplx_out.vertices)\n",
    "smpl_verts_c = apply_T_on_points(smpl_verts, T_w2c)\n",
    "smpl_joints_c = smpl_J_regressor @ smpl_verts_c  # (F, 24, 3)\n",
    "(smpl_joints_c - joints3d).abs().max()  # 2.8e-6\n",
    "\n",
    "# from hmr4d.utils.geo_transform import project_p2d\n",
    "# from hmr4d.utils.vis.vis_kpts import draw_kpts_cv2\n",
    "# p2d = project_p2d(smpl_joints_c, K, True) / 4  # we use ds4 image as visualization\n",
    "\n",
    "# imgs = []\n",
    "# for i in range(len(frames)):\n",
    "#     img = frames[i].numpy().copy()\n",
    "#     img = draw_kpts_cv2(img, p2d[i])\n",
    "#     imgs.append(img)\n",
    "# imageio.mimsave(\"tmp2.mp4\", imgs, fps=30, quality=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wis3d = make_wis3d(name=\"debug_wham_rich_gt\")\n",
    "\n",
    "for i in range(len(smpl_verts)):\n",
    "    wis3d.set_scene_id(i)\n",
    "    wis3d.add_point_cloud(smpl_verts[i], name=\"smpl_verts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/Gym_012_lunge1/cam_05'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[\"vid\"][index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GT from RICH dataset\n",
    "可以只用WHAM提供的pth，就能拿到我想要的结果！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/Gym_012_lunge1/cam_05: MOTION start/end-id=5/204 @ VIDEO(JPEG) start/end-id=2/225\n",
      "test/Gym_012_lunge1/cam_05: FRAME_ID start/end = 3/202\n"
     ]
    }
   ],
   "source": [
    "RICH_test_body_dir = Path(\"/mnt/data/Datasets/RICH/bodies/test_body\")\n",
    "seqname = vid.split(\"/\")[1]\n",
    "sub_id = seqname.split(\"_\")[1]\n",
    "pkl_paths = list(sorted((RICH_test_body_dir / seqname).glob(f\"*/{sub_id}.pkl\")))\n",
    "m_sid = int(pkl_paths[0].parent.name)  # 5\n",
    "m_eid = int(pkl_paths[-1].parent.name)  # 204\n",
    "\n",
    "rich_video_dir = Path(\"inputs/RICH/hmr4d_support/video\")\n",
    "meta_txt = rich_video_dir / vid / \"meta.txt\"  # 表示images_ds4里，第一个和最后一个jpg的文件名，比如[2, 255]一共有254帧，一个seq的不同cam可能不一样\n",
    "v_sfid, v_efid = [int(x) for x in meta_txt.open(\"r\").readline().split()]\n",
    "\n",
    "# NOTE: Understand the frame-id in WHAM, the Motion, the Images\n",
    "print(f\"{vid}: MOTION start/end-id={m_sid}/{m_eid} @ VIDEO(JPEG) start/end-id={v_sfid}/{v_efid}\")\n",
    "frame_id = labels[\"frame_id\"][index][1:]\n",
    "print(f\"{vid}: FRAME_ID start/end = {frame_id[0]}/{frame_id[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------: the transl in labels is the same as that in original pkl\n",
      "[[ 0.93521154 -0.01478121  4.3353    ]]\n",
      "tensor([ 0.9352, -0.0148,  4.3353])\n",
      "\n",
      "-------: the local pose in labels is the same as that in original pkl\n",
      "differences: tensor(0.)\n",
      "\n",
      "-------: the global pose in labels @ cam-pose-R is the same as that in original pkl\n",
      "[[-1.5348296  -0.30412704  2.5799594 ]]\n",
      "tensor([-1.5348, -0.3041,  2.5800])\n",
      "difference: tensor([[ 0.0000e+00,  2.3842e-07, -2.3842e-07]])\n"
     ]
    }
   ],
   "source": [
    "data = joblib.load(pkl_paths[0])\n",
    "print(\"-------: the transl in labels is the same as that in original pkl\")\n",
    "print(data[\"transl\"])\n",
    "print(labels[\"transl\"][index][1])\n",
    "print(\"\")\n",
    "\n",
    "# Check pose\n",
    "print(\"-------: the local pose in labels is the same as that in original pkl\")\n",
    "print(\"differences:\", (labels[\"pose\"][index][1][3:-6] - torch.from_numpy(data[\"body_pose\"][0])).abs().sum())\n",
    "print(\"\")\n",
    "\n",
    "# Global orient\n",
    "print(\"-------: the global pose in labels @ cam-pose-R is the same as that in original pkl\")\n",
    "print(data[\"global_orient\"])\n",
    "# print(labels[\"pose\"][index][1][:3])\n",
    "from pytorch3d.transforms import axis_angle_to_matrix, matrix_to_axis_angle\n",
    "\n",
    "cam_pose = labels[\"cam_poses\"][index][1][:3, :3]\n",
    "gpose_label = axis_angle_to_matrix(labels[\"pose\"][index][1][:3]).squeeze(0).numpy()\n",
    "print(matrix_to_axis_angle(cam_pose.T @ gpose_label))\n",
    "print(\"difference:\", matrix_to_axis_angle(cam_pose.T @ gpose_label) - torch.from_numpy(data[\"global_orient\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

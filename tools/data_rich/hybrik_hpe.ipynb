{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir('../../')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import imageio\n",
    "import decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.network.hpe.hybrik import HRNetSMPLCam, cfg_model, MEAN, STD\n",
    "CKPT = \"inputs/checkpoints/hybrik/hybrik_hrnet48_w3dpw.pth\"\n",
    "\n",
    "hybrik_model = HRNetSMPLCam(**cfg_model).eval()\n",
    "save_dict = torch.load(CKPT, map_location='cpu')\n",
    "hybrik_model.load_state_dict(save_dict, strict=False)\n",
    "hybrik_model = hybrik_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"inputs/RICH/sahmr_support/test_split/image/Gym_010_cooking1_4_00356_10.png\"\n",
    "img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)  # [H, W, C]\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img_ts = torch.from_numpy(img).float().unsqueeze(0).permute(0, 3, 1, 2) / 255.0  # [1, C, H, W]\n",
    "img_input = (img_ts - MEAN) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_kpts = hybrik_model(img_input)\n",
    "    pred_joints24_xy = pred_kpts[:, :, :2] * 256\n",
    "\n",
    "from hmr4d.utils.vis.vis_kpts import draw_kpts_cv2\n",
    "img_ = draw_kpts_cv2(img, pred_joints24_xy[0].cpu().numpy())\n",
    "\n",
    "# make plt fig smaller\n",
    "fig = plt.figure(figsize=(3, 3), dpi=70)\n",
    "plt.axis('off')\n",
    "plt.imshow(img_)\n",
    "# remove axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output all estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "rich_test_vit_path = \"inputs/RICH/eval_support/rich_test_vit.pth\"\n",
    "labels = joblib.load(rich_test_vit_path)\n",
    "rich_video_dir = Path(\"inputs/RICH/hmr4d_support/video\")\n",
    "\n",
    "\n",
    "vid_to_pred_kpts = {}\n",
    "for index in tqdm(range(len(labels[\"vid\"]))):\n",
    "    bbox = labels[\"bbox\"][index][1:]  # (F, 3) # 3: (x, y, s)\n",
    "    frame_id = labels[\"frame_id\"][index][1:]  # (F, )\n",
    "    vid = labels[\"vid\"][index]\n",
    "\n",
    "    decord.bridge.set_bridge(\"torch\") \n",
    "    vr = decord.VideoReader(str(rich_video_dir / vid / \"video.mp4\"))\n",
    "    frames = vr.get_batch(list(frame_id.numpy()))  # (F, 752, 1024, 3)\n",
    "\n",
    "\n",
    "    # Crop images according to bbox\n",
    "    bbox_src = torch.stack(\n",
    "        [\n",
    "            torch.stack([bbox[:, 0] - bbox[:, 2] / 2, bbox[:, 1] - bbox[:, 2] / 2], dim=-1),\n",
    "            torch.stack([bbox[:, 0] + bbox[:, 2] / 2, bbox[:, 1] - bbox[:, 2] / 2], dim=-1),\n",
    "            bbox[:, :2],\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    bbox_src = bbox_src.numpy()  # (F, 3, 2)\n",
    "    dst_size = 256\n",
    "    bbox_dst = np.array([[0, 0], [dst_size - 1, 0], [dst_size / 2 - 0.5, dst_size / 2 - 0.5]], dtype=np.float32)\n",
    "    As = [cv2.getAffineTransform(src / 4, bbox_dst) for src in bbox_src]\n",
    "    img_crops = [\n",
    "        cv2.warpAffine(frames[i].numpy(), As[i], (dst_size, dst_size), flags=cv2.INTER_LINEAR) for i in range(len(As))\n",
    "    ]\n",
    "\n",
    "    img_inputs = torch.stack([torch.from_numpy(img).permute(2, 0, 1) for img in img_crops], dim=0).float() / 255.0\n",
    "    img_inputs = (img_inputs - MEAN) / STD\n",
    "    img_inputs = img_inputs.cuda()\n",
    "\n",
    "    pred_kpts = []\n",
    "    batch_size = 64\n",
    "    for i in tqdm(range(0, len(img_inputs), batch_size)):\n",
    "        with torch.no_grad():\n",
    "            pred_kpts.append(hybrik_model(img_inputs[i:i+batch_size]))\n",
    "    pred_kpts = torch.cat(pred_kpts, dim=0)\n",
    "\n",
    "    # pred_joints24_xy = pred_kpts[:, :, :2] * 256\n",
    "\n",
    "    vid_to_pred_kpts[vid] = pred_kpts.clone().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vid_to_pred_kpts, \"inputs/RICH/eval_support/hybrik_pred_kpts.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.vis.vis_kpts import draw_kpts_cv2\n",
    "\n",
    "imgs_keypoints_overlay = []\n",
    "for i in range(len(img_crops)):\n",
    "    img = img_crops[i].copy()\n",
    "    img_ = draw_kpts_cv2(img, pred_joints24_xy[i].cpu().numpy())\n",
    "    imgs_keypoints_overlay.append(img_)\n",
    "\n",
    "# imageio.mimsave(\"imgs_keypoints_overlay.mp4\", imgs_keypoints_overlay, fps=30, quality=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check WAHM BBX behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "rich_test_vit_path = \"inputs/RICH/eval_support/rich_test_vit.pth\"\n",
    "labels = joblib.load(rich_test_vit_path)\n",
    "rich_video_dir = Path(\"inputs/RICH/hmr4d_support/video\")\n",
    "\n",
    "\n",
    "vid_to_pred_kpts = {}\n",
    "for index in tqdm(range(len(labels[\"vid\"]))):\n",
    "    bbox = labels[\"bbox\"][index][1:]  # (F, 3) # 3: (x, y, s)\n",
    "    frame_id = labels[\"frame_id\"][index][1:]  # (F, )\n",
    "    vid = labels[\"vid\"][index]\n",
    "\n",
    "    decord.bridge.set_bridge(\"torch\") \n",
    "    vr = decord.VideoReader(str(rich_video_dir / vid / \"video.mp4\"))\n",
    "    frames = vr.get_batch(list(frame_id.numpy()))  # (F, 752, 1024, 3)\n",
    "\n",
    "\n",
    "    # Crop images according to bbox\n",
    "    bbox_src = torch.stack(\n",
    "        [\n",
    "            torch.stack([bbox[:, 0] - bbox[:, 2] / 2, bbox[:, 1] - bbox[:, 2] / 2], dim=-1),\n",
    "            torch.stack([bbox[:, 0] + bbox[:, 2] / 2, bbox[:, 1] - bbox[:, 2] / 2], dim=-1),\n",
    "            bbox[:, :2],\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    bbox_src = bbox_src.numpy()  # (F, 3, 2)\n",
    "    dst_size = 256\n",
    "    bbox_dst = np.array([[0, 0], [dst_size - 1, 0], [dst_size / 2 - 0.5, dst_size / 2 - 0.5]], dtype=np.float32)\n",
    "    As = [cv2.getAffineTransform(src / 4, bbox_dst) for src in bbox_src]\n",
    "    img_crops = [\n",
    "        cv2.warpAffine(frames[i].numpy(), As[i], (dst_size, dst_size), flags=cv2.INTER_LINEAR) for i in range(len(As))\n",
    "    ]\n",
    "\n",
    "\n",
    "    out_dir = Path(\"tmp_wham_bbx_videos\")\n",
    "    out_fn = out_dir / f\"{vid.replace('/', '_')}.mp4\"\n",
    "    imageio.mimsave(out_fn, img_crops, fps=30, quality=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Kpt Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_dir = Path(\"inputs/RICH\")\n",
    "kpts = torch.load(rich_dir / \"eval_support/hybrik_pred_kpts.pth\")\n",
    "\n",
    "# Bbx information\n",
    "import joblib\n",
    "rich_test_vit_path = \"inputs/RICH/eval_support/rich_test_vit.pth\"\n",
    "labels = joblib.load(rich_test_vit_path)\n",
    "rich_video_dir = Path(\"inputs/RICH/hmr4d_support/video\")\n",
    "\n",
    "index_to_vid = {i: vid for i, vid in enumerate(labels[\"vid\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbx_images(labels, index, rich_video_dir, dst_size=256):\n",
    "    bbox = labels[\"bbox\"][index][1:]  # (F, 3) # 3: (x, y, s)\n",
    "    frame_id = labels[\"frame_id\"][index][1:]  # (F, )\n",
    "    vid = labels[\"vid\"][index]\n",
    "    decord.bridge.set_bridge(\"torch\") \n",
    "    vr = decord.VideoReader(str(rich_video_dir / vid / \"video.mp4\"))\n",
    "    frames = vr.get_batch(list(frame_id.numpy()))  # (F, 752, 1024, 3)  \n",
    "\n",
    "    # crop images according to bbox\n",
    "    bbox_src = torch.stack(\n",
    "        [\n",
    "            torch.stack([bbox[:, 0] - bbox[:, 2] / 2, bbox[:, 1] - bbox[:, 2] / 2], dim=-1),\n",
    "            torch.stack([bbox[:, 0] + bbox[:, 2] / 2, bbox[:, 1] - bbox[:, 2] / 2], dim=-1),\n",
    "            bbox[:, :2],\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    bbox_src = bbox_src.numpy()  # (F, 3, 2)\n",
    "    bbox_dst = np.array([[0, 0], [dst_size - 1, 0], [dst_size / 2 - 0.5, dst_size / 2 - 0.5]], dtype=np.float32)\n",
    "    As = [cv2.getAffineTransform(src / 4, bbox_dst) for src in bbox_src]\n",
    "    img_crops = [\n",
    "        cv2.warpAffine(frames[i].numpy(), As[i], (dst_size, dst_size), flags=cv2.INTER_LINEAR) for i in range(len(As))\n",
    "    ]\n",
    "\n",
    "    return img_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 22, 2)\n"
     ]
    }
   ],
   "source": [
    "dst_size = 256\n",
    "index = 20\n",
    "vid = index_to_vid[index]\n",
    "pred_kpts = kpts[vid][:, :22, :2] * dst_size # [F, 22, 2]\n",
    "print(pred_kpts.shape)\n",
    "img_crops = get_bbx_images(labels, index, rich_video_dir, dst_size=dst_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.vis.vis_kpts import draw_kpts_cv2\n",
    "\n",
    "imgs_keypoints_overlay = []\n",
    "for i in range(len(img_crops)):\n",
    "    img = img_crops[i].copy()\n",
    "    img_ = draw_kpts_cv2(img, pred_kpts[i])\n",
    "    imgs_keypoints_overlay.append(img_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave(\"imgs_keypoints_overlay.mp4\", imgs_keypoints_overlay, fps=30, quality=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_out = {\"img_crops\": img_crops, \"pred_kpts\": pred_kpts}\n",
    "np.save(\"vis_out.npy\", vis_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir('../../')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import imageio\n",
    "import decord\n",
    "import joblib\n",
    "from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines\n",
    "from hmr4d.dataset.rich.rich_utils import get_w2az_sahmr, parse_seqname_info\n",
    "from hmr4d.utils.geo_transform import apply_T_on_points\n",
    "from hmr4d.utils.vis.vis_kpts import draw_kpts_cv2\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "这个文件用于得到SuperMotion的Mocap测试案例的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.smplx_utils import make_smplx\n",
    "from hmr4d.dataset.rich.rich_utils import get_cam2params, get_cam_key_wham_vid\n",
    "\n",
    "# dataset\n",
    "labels = joblib.load(\"inputs/RICH/eval_support/rich_test_vit.pth\")\n",
    "\n",
    "# 3D initialization (WHAM prediction)\n",
    "wham_outputs = torch.load(Path(\"inputs/RICH/eval_support/wham_output.pt\"))\n",
    "smpl_model = make_smplx(\"smpl\", gender=\"neutral\").cuda().eval()\n",
    "smpl_J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\")\n",
    "\n",
    "# 2D initialization (HybrIK prediction)\n",
    "hybrik_outpus = torch.load(\"inputs/RICH/eval_support/hybrik_pred_kpts.pth\")\n",
    "cam2params = get_cam2params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:07<00:00, 24.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from hmr4d.utils.geo_transform import transform_mat, compute_T_ayf2az\n",
    "from hmr4d.utils.geo_transform import ransac_PnP_batch, transform_mat, homo_points\n",
    "\n",
    "outputs = {\n",
    "    \"vid_to_c_p2d_cv\": {},\n",
    "    \"vid_to_T_ayfz2c\": {},\n",
    "    \"vid_to_pred_j3d_ayfz\": {},  # every frame is ayfz\n",
    "    \"vid_to_pred_j3d_ayfz1\": {},  # only first frame is ayfz\n",
    "}\n",
    "\n",
    "for index in tqdm(range(len(labels[\"vid\"]))):\n",
    "    vid = labels[\"vid\"][index]\n",
    "    bbox_xys = labels[\"bbox\"][index][1:]  # (F, 3)\n",
    "    cam_key = get_cam_key_wham_vid(vid)\n",
    "\n",
    "    # <======= Load Psrediction\n",
    "    wham_pred = wham_outputs[vid]\n",
    "    params = {\n",
    "        \"body_pose\": wham_pred[\"poses_body\"].reshape(-1, 23, 3, 3),\n",
    "        \"global_orient\": wham_pred[\"poses_root_world\"].reshape(-1, 1, 3, 3),\n",
    "        \"betas\": wham_pred[\"betas\"].reshape(-1, 10),\n",
    "        \"transl\": wham_pred[\"trans_world\"].reshape(-1, 3),\n",
    "    }\n",
    "    params = {k: v.cuda() for k, v in params.items()}\n",
    "    pred_smpl_out = smpl_model.forward(**params, pose2rot=False)\n",
    "    pred_smpl_verts = pred_smpl_out.vertices.cpu()  # (F, 6890, 3)\n",
    "    pred_j3d_glob = smpl_J_regressor @ pred_smpl_verts  # (F, 24, 3)\n",
    "\n",
    "    pred_01_kpts2d = torch.from_numpy(hybrik_outpus[vid])  # (F, 24, 3)\n",
    "    _, K = cam2params[cam_key]\n",
    "    pred_i_kpts = (pred_01_kpts2d[:, :, :2] - 0.5) * bbox_xys[:, None, [-1]] + bbox_xys[:, None, :2]\n",
    "\n",
    "    # <======= Get pred_j3d_ayfz, T_ayfz2c, c_p2d_cv\n",
    "    # ydown(WHAM) -> zup -> az (align-ground)\n",
    "    R_ydown2zup = torch.tensor([[1, 0, 0], [0, 0, 1], [0, -1, 0]], dtype=torch.float32)\n",
    "    lowest_point = (pred_smpl_verts @ R_ydown2zup.T)[:, :, 2].min(dim=-1)[0]  # TODO: maybe use the first frames?\n",
    "    lowest_point = torch.mean(lowest_point)\n",
    "    t_zup2az = torch.tensor([0, 0, -lowest_point], dtype=torch.float32)\n",
    "\n",
    "    T_ydown2az = transform_mat(R_ydown2zup, t_zup2az)\n",
    "    pred_j3d_az = apply_T_on_points(pred_j3d_glob, T_ydown2az)  # (F, 24, 3)\n",
    "\n",
    "    # TODO: this is very stupid, since every interval frames will have different T_az2ayfz\n",
    "    # Wrong, but keep here:\n",
    "    # T_az2ayfz = compute_T_ayf2az(pred_j3d_az[None, 0], inverse=True)[0]  # (4, 4)\n",
    "    # pred_j3d_ayfz = apply_T_on_points(pred_j3d_az, T_az2ayfz)  # (F, 24, 3)\n",
    "    # fit_R, fit_t = ransac_PnP_batch(K.numpy()[None], pred_i_kpts.numpy(), pred_j3d_ayfz.numpy(), err_thr=10)\n",
    "    # T_ayfz2c = transform_mat(torch.FloatTensor(fit_R), torch.FloatTensor(fit_t))\n",
    "\n",
    "    # j3d_ayfz is every frame ayfz V.S. j3d_ayfz1 is only first frame ayfz\n",
    "    T_az2ayfz = compute_T_ayf2az(pred_j3d_az, inverse=True)  # (F, 4, 4)\n",
    "    pred_j3d_ayfz1 = apply_T_on_points(pred_j3d_az, T_az2ayfz[0])  # (F, 24, 3), every frame is an ayfz\n",
    "    pred_j3d_ayfz = apply_T_on_points(pred_j3d_az, T_az2ayfz)  # (F, 24, 3), every frame is an ayfz\n",
    "    Ks = K[None].repeat(pred_j3d_ayfz.shape[0], 1, 1)\n",
    "    fit_R, fit_t = ransac_PnP_batch(Ks.numpy(), pred_i_kpts.numpy(), pred_j3d_ayfz.numpy(), err_thr=10)\n",
    "    T_ayfz2c = transform_mat(torch.FloatTensor(fit_R), torch.FloatTensor(fit_t))  # (F, 4, 4)\n",
    "\n",
    "    # Get c_p2d_cv\n",
    "    c_p2d_cv = (pred_i_kpts - K[:2, 2]) / torch.tensor([K[0, 0], K[1, 1]])\n",
    "\n",
    "    # Add to dictionary\n",
    "    outputs[\"vid_to_c_p2d_cv\"][vid] = c_p2d_cv.cpu()\n",
    "    outputs[\"vid_to_T_ayfz2c\"][vid] = T_ayfz2c.cpu()\n",
    "    outputs[\"vid_to_pred_j3d_ayfz\"][vid] = pred_j3d_ayfz.cpu()\n",
    "    outputs[\"vid_to_pred_j3d_ayfz1\"][vid] = pred_j3d_ayfz1.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pred_j3d_ayfz, T_ayfz2c, c_p2d_cv\n",
    "torch.save(outputs, \"/home/shenzehong/Code/HMR-4D/inputs/RICH/eval_support/sm_rich_mocap_input.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wis3d = make_wis3d(name=\"mocap-setup\")\n",
    "# wis3d.add_point_cloud(pred_[0], name=\"zup\")\n",
    "# wis3d.add_point_cloud(pred_j3d_az[0], name='az')\n",
    "add_motion_as_lines(pred_j3d_az, wis3d, name=\"az\")\n",
    "add_motion_as_lines(pred_j3d_ayfz, wis3d, name=\"ayfz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from hydra import initialize_config_module, compose\n",
    "with initialize_config_module(version_base=\"1.3\", config_module=f\"hmr4d.configs\"):\n",
    "    overrides = ['exp=motion3d_prior/baseline/net_bertlike', \"global/task=supermotion/test_mocap_rich_prior3d\", \n",
    "                 f'ckpt_path=inputs/checkpoints/trained/m3dp_bertlike_e099-s105300.ckpt']\n",
    "    cfg = compose(config_name='train', overrides=overrides)\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from hmr4d.utils.net_utils import load_pretrained_model\n",
    "\n",
    "model = instantiate(cfg.model, _recursive_=False)\n",
    "load_pretrained_model(model, cfg.ckpt_path, cfg.ckpt_type)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "\n",
    "## Dataset to align with WHAM\n",
    "idx2meta = []\n",
    "for index, vid in enumerate(labels[\"vid\"]):\n",
    "    chunk_length = 100\n",
    "    seq_length = len(labels[\"frame_id\"][index][1:])\n",
    "    for start in range(0, seq_length - chunk_length, chunk_length):\n",
    "        end = start + chunk_length\n",
    "        if start + 2 * chunk_length > seq_length:  # last one\n",
    "            end = seq_length  # [start, end)\n",
    "        idx2meta.append((index, vid, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "index, vid, start, end = idx2meta[idx]\n",
    "\n",
    "c_p2d_cv = outputs[\"vid_to_c_p2d_cv\"][vid][start:end]\n",
    "T_ayfz2c = outputs[\"vid_to_T_ayfz2c\"][vid]\n",
    "pred_j3d_ayfz = outputs[\"vid_to_pred_j3d_ayfz\"][vid][start:end]\n",
    "\n",
    "B = 1\n",
    "batch = {\n",
    "    \"B\": B,\n",
    "    \"gt_ayfz_motion\": \"\",\n",
    "    \"length\": torch.tensor([100]),\n",
    "    \"text\": \"\",\n",
    "    \"img_seq\": torch.zeros((B, 15, 3, 224, 224)),  \n",
    "    \"img_seq_fid\": torch.ones(B, 15).long() * -1,\n",
    "    \"task\": \"CAP\",\n",
    "    \"T_ayfz2c\": T_ayfz2c,\n",
    "    \"c_p2d_cv\": c_p2d_cv[None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items() }\n",
    "outputs = model.validation_step(batch, 0)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
